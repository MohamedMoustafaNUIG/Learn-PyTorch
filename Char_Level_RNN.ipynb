{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open text file and read in data as `text`\n",
    "with open('data/anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Chapter 1\\n\\n\\nHappy families are all alike; every unhappy family is unhappy in its own\\nway.\\n\\nEverythin'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chars = tuple(set(text))\n",
    "int2char = dict(enumerate(chars))\n",
    "char2int = {ch: ii for ii, ch in int2char.items()}\n",
    "\n",
    "# encode the text\n",
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([59, 65, 43, 14, 82, 63, 37, 33, 53, 57, 57, 57, 23, 43, 14, 14, 17,\n",
       "       33, 64, 43, 46, 80, 50, 80, 63, 49, 33, 43, 37, 63, 33, 43, 50, 50,\n",
       "       33, 43, 50, 80, 24, 63, 58, 33, 63, 56, 63, 37, 17, 33, 39, 34, 65,\n",
       "       43, 14, 14, 17, 33, 64, 43, 46, 80, 50, 17, 33, 80, 49, 33, 39, 34,\n",
       "       65, 43, 14, 14, 17, 33, 80, 34, 33, 80, 82, 49, 33, 11, 21, 34, 57,\n",
       "       21, 43, 17,  4, 57, 57, 61, 56, 63, 37, 17, 82, 65, 80, 34])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n",
    "    # Fill the appropriate elements with ones\n",
    "    \n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    print(one_hot)\n",
    "    print(\"\")\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "[[[0. 0. 0. 1. 0. 0. 0. 0.]\n",
      "  [0. 0. 0. 0. 0. 1. 0. 0.]\n",
      "  [0. 1. 0. 0. 0. 0. 0. 0.]]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# check that the function works as expected\n",
    "test_seq = np.array([[3, 5, 1]])\n",
    "one_hot = one_hot_encode(test_seq, 8)\n",
    "\n",
    "#print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches = get_batches(encoded, 8, 50)\n",
    "x, y = next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      " [[36 23 43 35 63 50 17  3 77 32]\n",
      " [28 15  2  3 63 23 43 63  3 43]\n",
      " [50  2 11  3 15 17  3 43  3 62]\n",
      " [28  3 63 23 50  3 29 23 69 50]\n",
      " [ 3 28 43  6  3 23 50 17  3 63]\n",
      " [29  1 28 28 69 15  2  3 43  2]\n",
      " [ 3 46  2  2 43  3 23 43 11  3]\n",
      " [10 57 16 15  2 28 66 18 58  3]]\n",
      "\n",
      "y\n",
      " [[23 43 35 63 50 17  3 77 32 32]\n",
      " [15  2  3 63 23 43 63  3 43 63]\n",
      " [ 2 11  3 15 17  3 43  3 62 15]\n",
      " [ 3 63 23 50  3 29 23 69 50 62]\n",
      " [28 43  6  3 23 50 17  3 63 50]\n",
      " [ 1 28 28 69 15  2  3 43  2 11]\n",
      " [46  2  2 43  3 23 43 11  3 28]\n",
      " [57 16 15  2 28 66 18 58  3 75]]\n"
     ]
    }
   ],
   "source": [
    "print('x\\n', x[:10, :10])\n",
    "print('\\ny\\n', y[:10, :10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on GPU!\n"
     ]
    }
   ],
   "source": [
    "# check if GPU is available\n",
    "train_on_gpu = torch.cuda.is_available()\n",
    "if(train_on_gpu):\n",
    "    print('Training on GPU!')\n",
    "else: \n",
    "    print('No GPU available, training on CPU; consider making n_epochs very small.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, tokens, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        # creating character dictionaries\n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        ## TODO: define the LSTM\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        \n",
    "        ## TODO: define a dropout layer\n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        \n",
    "        ## TODO: define the final, fully-connected output layer\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "      \n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        ''' Forward pass through the network. \n",
    "            These inputs are x, and the hidden/cell state `hidden`. '''\n",
    "                \n",
    "        ## TODO: Get the outputs and the new hidden state from the lstm\n",
    "        r_output, hidden = self.lstm(x, hidden)\n",
    "        \n",
    "        ## TODO: pass through a dropout layer\n",
    "        out = self.dropout(r_output)\n",
    "        \n",
    "        # Stack up LSTM outputs using view\n",
    "        # you may need to use contiguous to reshape the output\n",
    "        out = out.contiguous().view(-1, self.n_hidden)\n",
    "        \n",
    "        ## TODO: put x through the fully-connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        # return the final output and the hidden state\n",
    "        return out, hidden\n",
    "    \n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        \n",
    "        if (train_on_gpu):\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n",
    "                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n",
    "        else:\n",
    "            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n",
    "                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n",
    "        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n",
    "    ''' Training a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        batch_size: Number of mini-sequences per mini-batch, aka batch size\n",
    "        seq_length: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    net.train()\n",
    "    \n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        # initialize hidden state\n",
    "        h = net.init_hidden(batch_size)\n",
    "        \n",
    "        for x, y in get_batches(data, batch_size, seq_length):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            if(train_on_gpu):\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([each.data for each in h])\n",
    "\n",
    "            # zero accumulated gradients\n",
    "            net.zero_grad()\n",
    "            \n",
    "            # get the output from the model\n",
    "            output, h = net(inputs, h)\n",
    "            \n",
    "            # calculate the loss and perform backprop\n",
    "            loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "            loss.backward()\n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "            opt.step()\n",
    "            \n",
    "            # loss stats\n",
    "            if counter % print_every == 0:\n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(batch_size)\n",
    "                val_losses = []\n",
    "                net.eval()\n",
    "                for x, y in get_batches(val_data, batch_size, seq_length):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([each.data for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = x, y\n",
    "                    if(train_on_gpu):\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n",
    "                \n",
    "                    val_losses.append(val_loss.item())\n",
    "                \n",
    "                net.train() # reset to train mode after iterationg through validation data\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.item()),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CharRNN(\n",
      "  (lstm): LSTM(83, 512, num_layers=2, batch_first=True, dropout=0.5)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=83, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#define and print the net\n",
    "n_hidden=512\n",
    "n_layers=2\n",
    "\n",
    "net = CharRNN(chars, n_hidden, n_layers)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/20... Step: 10... Loss: 3.2825... Val Loss: 3.2288\n",
      "Epoch: 1/20... Step: 20... Loss: 3.1557... Val Loss: 3.1404\n",
      "Epoch: 1/20... Step: 30... Loss: 3.1452... Val Loss: 3.1261\n",
      "Epoch: 1/20... Step: 40... Loss: 3.1145... Val Loss: 3.1197\n",
      "Epoch: 1/20... Step: 50... Loss: 3.1462... Val Loss: 3.1180\n",
      "Epoch: 1/20... Step: 60... Loss: 3.1209... Val Loss: 3.1163\n",
      "Epoch: 1/20... Step: 70... Loss: 3.1102... Val Loss: 3.1151\n",
      "Epoch: 1/20... Step: 80... Loss: 3.1246... Val Loss: 3.1120\n",
      "Epoch: 1/20... Step: 90... Loss: 3.1220... Val Loss: 3.1052\n",
      "Epoch: 1/20... Step: 100... Loss: 3.0987... Val Loss: 3.0880\n",
      "Epoch: 1/20... Step: 110... Loss: 3.0674... Val Loss: 3.0485\n",
      "Epoch: 1/20... Step: 120... Loss: 2.9788... Val Loss: 2.9743\n",
      "Epoch: 1/20... Step: 130... Loss: 2.8821... Val Loss: 2.9757\n",
      "Epoch: 2/20... Step: 140... Loss: 2.8556... Val Loss: 2.7967\n",
      "Epoch: 2/20... Step: 150... Loss: 2.7248... Val Loss: 2.6726\n",
      "Epoch: 2/20... Step: 160... Loss: 2.6352... Val Loss: 2.5918\n",
      "Epoch: 2/20... Step: 170... Loss: 2.5392... Val Loss: 2.5337\n",
      "Epoch: 2/20... Step: 180... Loss: 2.5110... Val Loss: 2.4905\n",
      "Epoch: 2/20... Step: 190... Loss: 2.4531... Val Loss: 2.4455\n",
      "Epoch: 2/20... Step: 200... Loss: 2.4460... Val Loss: 2.4588\n",
      "Epoch: 2/20... Step: 210... Loss: 2.4060... Val Loss: 2.3812\n",
      "Epoch: 2/20... Step: 220... Loss: 2.3732... Val Loss: 2.3611\n",
      "Epoch: 2/20... Step: 230... Loss: 2.3474... Val Loss: 2.3331\n",
      "Epoch: 2/20... Step: 240... Loss: 2.3360... Val Loss: 2.3032\n",
      "Epoch: 2/20... Step: 250... Loss: 2.2751... Val Loss: 2.2855\n",
      "Epoch: 2/20... Step: 260... Loss: 2.2438... Val Loss: 2.2489\n",
      "Epoch: 2/20... Step: 270... Loss: 2.2376... Val Loss: 2.2141\n",
      "Epoch: 3/20... Step: 280... Loss: 2.2388... Val Loss: 2.1883\n",
      "Epoch: 3/20... Step: 290... Loss: 2.2052... Val Loss: 2.1597\n",
      "Epoch: 3/20... Step: 300... Loss: 2.1673... Val Loss: 2.1404\n",
      "Epoch: 3/20... Step: 310... Loss: 2.1515... Val Loss: 2.1255\n",
      "Epoch: 3/20... Step: 320... Loss: 2.1201... Val Loss: 2.1011\n",
      "Epoch: 3/20... Step: 330... Loss: 2.0909... Val Loss: 2.0813\n",
      "Epoch: 3/20... Step: 340... Loss: 2.1017... Val Loss: 2.0656\n",
      "Epoch: 3/20... Step: 350... Loss: 2.0858... Val Loss: 2.0406\n",
      "Epoch: 3/20... Step: 360... Loss: 2.0181... Val Loss: 2.0253\n",
      "Epoch: 3/20... Step: 370... Loss: 2.0441... Val Loss: 2.0060\n",
      "Epoch: 3/20... Step: 380... Loss: 2.0112... Val Loss: 1.9885\n",
      "Epoch: 3/20... Step: 390... Loss: 1.9889... Val Loss: 1.9734\n",
      "Epoch: 3/20... Step: 400... Loss: 1.9609... Val Loss: 1.9543\n",
      "Epoch: 3/20... Step: 410... Loss: 1.9721... Val Loss: 1.9435\n",
      "Epoch: 4/20... Step: 420... Loss: 1.9681... Val Loss: 1.9295\n",
      "Epoch: 4/20... Step: 430... Loss: 1.9478... Val Loss: 1.9145\n",
      "Epoch: 4/20... Step: 440... Loss: 1.9294... Val Loss: 1.9034\n",
      "Epoch: 4/20... Step: 450... Loss: 1.8712... Val Loss: 1.8864\n",
      "Epoch: 4/20... Step: 460... Loss: 1.8552... Val Loss: 1.8728\n",
      "Epoch: 4/20... Step: 470... Loss: 1.8926... Val Loss: 1.8697\n",
      "Epoch: 4/20... Step: 480... Loss: 1.8667... Val Loss: 1.8492\n",
      "Epoch: 4/20... Step: 490... Loss: 1.8672... Val Loss: 1.8425\n",
      "Epoch: 4/20... Step: 500... Loss: 1.8711... Val Loss: 1.8266\n",
      "Epoch: 4/20... Step: 510... Loss: 1.8471... Val Loss: 1.8175\n",
      "Epoch: 4/20... Step: 520... Loss: 1.8551... Val Loss: 1.8078\n",
      "Epoch: 4/20... Step: 530... Loss: 1.8131... Val Loss: 1.7976\n",
      "Epoch: 4/20... Step: 540... Loss: 1.7757... Val Loss: 1.7873\n",
      "Epoch: 4/20... Step: 550... Loss: 1.8272... Val Loss: 1.7757\n",
      "Epoch: 5/20... Step: 560... Loss: 1.7883... Val Loss: 1.7652\n",
      "Epoch: 5/20... Step: 570... Loss: 1.7863... Val Loss: 1.7542\n",
      "Epoch: 5/20... Step: 580... Loss: 1.7496... Val Loss: 1.7437\n",
      "Epoch: 5/20... Step: 590... Loss: 1.7559... Val Loss: 1.7338\n",
      "Epoch: 5/20... Step: 600... Loss: 1.7443... Val Loss: 1.7268\n",
      "Epoch: 5/20... Step: 610... Loss: 1.7236... Val Loss: 1.7186\n",
      "Epoch: 5/20... Step: 620... Loss: 1.7293... Val Loss: 1.7146\n",
      "Epoch: 5/20... Step: 630... Loss: 1.7427... Val Loss: 1.7070\n",
      "Epoch: 5/20... Step: 640... Loss: 1.7168... Val Loss: 1.6963\n",
      "Epoch: 5/20... Step: 650... Loss: 1.6927... Val Loss: 1.6856\n",
      "Epoch: 5/20... Step: 660... Loss: 1.6693... Val Loss: 1.6805\n",
      "Epoch: 5/20... Step: 670... Loss: 1.6961... Val Loss: 1.6761\n",
      "Epoch: 5/20... Step: 680... Loss: 1.7006... Val Loss: 1.6659\n",
      "Epoch: 5/20... Step: 690... Loss: 1.6694... Val Loss: 1.6655\n",
      "Epoch: 6/20... Step: 700... Loss: 1.6719... Val Loss: 1.6541\n",
      "Epoch: 6/20... Step: 710... Loss: 1.6592... Val Loss: 1.6486\n",
      "Epoch: 6/20... Step: 720... Loss: 1.6488... Val Loss: 1.6416\n",
      "Epoch: 6/20... Step: 730... Loss: 1.6659... Val Loss: 1.6351\n",
      "Epoch: 6/20... Step: 740... Loss: 1.6329... Val Loss: 1.6326\n",
      "Epoch: 6/20... Step: 750... Loss: 1.6063... Val Loss: 1.6241\n",
      "Epoch: 6/20... Step: 760... Loss: 1.6469... Val Loss: 1.6193\n",
      "Epoch: 6/20... Step: 770... Loss: 1.6257... Val Loss: 1.6139\n",
      "Epoch: 6/20... Step: 780... Loss: 1.6091... Val Loss: 1.6076\n",
      "Epoch: 6/20... Step: 790... Loss: 1.5984... Val Loss: 1.6056\n",
      "Epoch: 6/20... Step: 800... Loss: 1.6194... Val Loss: 1.5994\n",
      "Epoch: 6/20... Step: 810... Loss: 1.6069... Val Loss: 1.5948\n",
      "Epoch: 6/20... Step: 820... Loss: 1.5628... Val Loss: 1.5905\n",
      "Epoch: 6/20... Step: 830... Loss: 1.6095... Val Loss: 1.5864\n",
      "Epoch: 7/20... Step: 840... Loss: 1.5567... Val Loss: 1.5818\n",
      "Epoch: 7/20... Step: 850... Loss: 1.5784... Val Loss: 1.5715\n",
      "Epoch: 7/20... Step: 860... Loss: 1.5677... Val Loss: 1.5664\n",
      "Epoch: 7/20... Step: 870... Loss: 1.5737... Val Loss: 1.5628\n",
      "Epoch: 7/20... Step: 880... Loss: 1.5762... Val Loss: 1.5607\n",
      "Epoch: 7/20... Step: 890... Loss: 1.5658... Val Loss: 1.5540\n",
      "Epoch: 7/20... Step: 900... Loss: 1.5440... Val Loss: 1.5539\n",
      "Epoch: 7/20... Step: 910... Loss: 1.5264... Val Loss: 1.5521\n",
      "Epoch: 7/20... Step: 920... Loss: 1.5562... Val Loss: 1.5464\n",
      "Epoch: 7/20... Step: 930... Loss: 1.5389... Val Loss: 1.5390\n",
      "Epoch: 7/20... Step: 940... Loss: 1.5401... Val Loss: 1.5366\n",
      "Epoch: 7/20... Step: 950... Loss: 1.5519... Val Loss: 1.5315\n",
      "Epoch: 7/20... Step: 960... Loss: 1.5466... Val Loss: 1.5295\n",
      "Epoch: 7/20... Step: 970... Loss: 1.5589... Val Loss: 1.5249\n",
      "Epoch: 8/20... Step: 980... Loss: 1.5249... Val Loss: 1.5255\n",
      "Epoch: 8/20... Step: 990... Loss: 1.5308... Val Loss: 1.5185\n",
      "Epoch: 8/20... Step: 1000... Loss: 1.5156... Val Loss: 1.5156\n",
      "Epoch: 8/20... Step: 1010... Loss: 1.5478... Val Loss: 1.5098\n",
      "Epoch: 8/20... Step: 1020... Loss: 1.5182... Val Loss: 1.5069\n",
      "Epoch: 8/20... Step: 1030... Loss: 1.5073... Val Loss: 1.5027\n",
      "Epoch: 8/20... Step: 1040... Loss: 1.5068... Val Loss: 1.5035\n",
      "Epoch: 8/20... Step: 1050... Loss: 1.4907... Val Loss: 1.4997\n",
      "Epoch: 8/20... Step: 1060... Loss: 1.4926... Val Loss: 1.4972\n",
      "Epoch: 8/20... Step: 1070... Loss: 1.4993... Val Loss: 1.4944\n",
      "Epoch: 8/20... Step: 1080... Loss: 1.4967... Val Loss: 1.4914\n",
      "Epoch: 8/20... Step: 1090... Loss: 1.4742... Val Loss: 1.4889\n",
      "Epoch: 8/20... Step: 1100... Loss: 1.4716... Val Loss: 1.4844\n",
      "Epoch: 8/20... Step: 1110... Loss: 1.4862... Val Loss: 1.4799\n",
      "Epoch: 9/20... Step: 1120... Loss: 1.4873... Val Loss: 1.4816\n",
      "Epoch: 9/20... Step: 1130... Loss: 1.4790... Val Loss: 1.4765\n",
      "Epoch: 9/20... Step: 1140... Loss: 1.4854... Val Loss: 1.4700\n",
      "Epoch: 9/20... Step: 1150... Loss: 1.5003... Val Loss: 1.4687\n",
      "Epoch: 9/20... Step: 1160... Loss: 1.4538... Val Loss: 1.4655\n",
      "Epoch: 9/20... Step: 1170... Loss: 1.4566... Val Loss: 1.4638\n",
      "Epoch: 9/20... Step: 1180... Loss: 1.4551... Val Loss: 1.4672\n",
      "Epoch: 9/20... Step: 1190... Loss: 1.4893... Val Loss: 1.4645\n",
      "Epoch: 9/20... Step: 1200... Loss: 1.4330... Val Loss: 1.4586\n",
      "Epoch: 9/20... Step: 1210... Loss: 1.4526... Val Loss: 1.4534\n",
      "Epoch: 9/20... Step: 1220... Loss: 1.4427... Val Loss: 1.4528\n",
      "Epoch: 9/20... Step: 1230... Loss: 1.4228... Val Loss: 1.4524\n",
      "Epoch: 9/20... Step: 1240... Loss: 1.4313... Val Loss: 1.4483\n",
      "Epoch: 9/20... Step: 1250... Loss: 1.4315... Val Loss: 1.4460\n",
      "Epoch: 10/20... Step: 1260... Loss: 1.4501... Val Loss: 1.4492\n",
      "Epoch: 10/20... Step: 1270... Loss: 1.4398... Val Loss: 1.4408\n",
      "Epoch: 10/20... Step: 1280... Loss: 1.4592... Val Loss: 1.4369\n",
      "Epoch: 10/20... Step: 1290... Loss: 1.4355... Val Loss: 1.4395\n",
      "Epoch: 10/20... Step: 1300... Loss: 1.4257... Val Loss: 1.4362\n",
      "Epoch: 10/20... Step: 1310... Loss: 1.4435... Val Loss: 1.4316\n",
      "Epoch: 10/20... Step: 1320... Loss: 1.4078... Val Loss: 1.4311\n",
      "Epoch: 10/20... Step: 1330... Loss: 1.4113... Val Loss: 1.4300\n",
      "Epoch: 10/20... Step: 1340... Loss: 1.3945... Val Loss: 1.4285\n",
      "Epoch: 10/20... Step: 1350... Loss: 1.3923... Val Loss: 1.4248\n",
      "Epoch: 10/20... Step: 1360... Loss: 1.3932... Val Loss: 1.4231\n",
      "Epoch: 10/20... Step: 1370... Loss: 1.3860... Val Loss: 1.4225\n",
      "Epoch: 10/20... Step: 1380... Loss: 1.4249... Val Loss: 1.4192\n",
      "Epoch: 10/20... Step: 1390... Loss: 1.4356... Val Loss: 1.4188\n",
      "Epoch: 11/20... Step: 1400... Loss: 1.4416... Val Loss: 1.4171\n",
      "Epoch: 11/20... Step: 1410... Loss: 1.4406... Val Loss: 1.4130\n",
      "Epoch: 11/20... Step: 1420... Loss: 1.4323... Val Loss: 1.4103\n",
      "Epoch: 11/20... Step: 1430... Loss: 1.4031... Val Loss: 1.4139\n",
      "Epoch: 11/20... Step: 1440... Loss: 1.4323... Val Loss: 1.4110\n",
      "Epoch: 11/20... Step: 1450... Loss: 1.3517... Val Loss: 1.4066\n",
      "Epoch: 11/20... Step: 1460... Loss: 1.3866... Val Loss: 1.4048\n",
      "Epoch: 11/20... Step: 1470... Loss: 1.3738... Val Loss: 1.4062\n",
      "Epoch: 11/20... Step: 1480... Loss: 1.3922... Val Loss: 1.3996\n",
      "Epoch: 11/20... Step: 1490... Loss: 1.3851... Val Loss: 1.4013\n",
      "Epoch: 11/20... Step: 1500... Loss: 1.3669... Val Loss: 1.4023\n",
      "Epoch: 11/20... Step: 1510... Loss: 1.3381... Val Loss: 1.3996\n",
      "Epoch: 11/20... Step: 1520... Loss: 1.3797... Val Loss: 1.3968\n",
      "Epoch: 12/20... Step: 1530... Loss: 1.4399... Val Loss: 1.3972\n",
      "Epoch: 12/20... Step: 1540... Loss: 1.3861... Val Loss: 1.3907\n",
      "Epoch: 12/20... Step: 1550... Loss: 1.3875... Val Loss: 1.3883\n",
      "Epoch: 12/20... Step: 1560... Loss: 1.4014... Val Loss: 1.3876\n",
      "Epoch: 12/20... Step: 1570... Loss: 1.3555... Val Loss: 1.3846\n",
      "Epoch: 12/20... Step: 1580... Loss: 1.3332... Val Loss: 1.3883\n",
      "Epoch: 12/20... Step: 1590... Loss: 1.3259... Val Loss: 1.3838\n",
      "Epoch: 12/20... Step: 1600... Loss: 1.3577... Val Loss: 1.3805\n",
      "Epoch: 12/20... Step: 1610... Loss: 1.3420... Val Loss: 1.3854\n",
      "Epoch: 12/20... Step: 1620... Loss: 1.3337... Val Loss: 1.3765\n",
      "Epoch: 12/20... Step: 1630... Loss: 1.3661... Val Loss: 1.3751\n",
      "Epoch: 12/20... Step: 1640... Loss: 1.3485... Val Loss: 1.3774\n",
      "Epoch: 12/20... Step: 1650... Loss: 1.3209... Val Loss: 1.3767\n",
      "Epoch: 12/20... Step: 1660... Loss: 1.3745... Val Loss: 1.3741\n",
      "Epoch: 13/20... Step: 1670... Loss: 1.3480... Val Loss: 1.3742\n",
      "Epoch: 13/20... Step: 1680... Loss: 1.3517... Val Loss: 1.3681\n",
      "Epoch: 13/20... Step: 1690... Loss: 1.3392... Val Loss: 1.3683\n",
      "Epoch: 13/20... Step: 1700... Loss: 1.3319... Val Loss: 1.3663\n",
      "Epoch: 13/20... Step: 1710... Loss: 1.3092... Val Loss: 1.3625\n",
      "Epoch: 13/20... Step: 1720... Loss: 1.3204... Val Loss: 1.3676\n",
      "Epoch: 13/20... Step: 1730... Loss: 1.3482... Val Loss: 1.3587\n",
      "Epoch: 13/20... Step: 1740... Loss: 1.3274... Val Loss: 1.3543\n",
      "Epoch: 13/20... Step: 1750... Loss: 1.2942... Val Loss: 1.3629\n",
      "Epoch: 13/20... Step: 1760... Loss: 1.3237... Val Loss: 1.3559\n",
      "Epoch: 13/20... Step: 1770... Loss: 1.3433... Val Loss: 1.3535\n",
      "Epoch: 13/20... Step: 1780... Loss: 1.3209... Val Loss: 1.3509\n",
      "Epoch: 13/20... Step: 1790... Loss: 1.2984... Val Loss: 1.3537\n",
      "Epoch: 13/20... Step: 1800... Loss: 1.3367... Val Loss: 1.3480\n",
      "Epoch: 14/20... Step: 1810... Loss: 1.3257... Val Loss: 1.3530\n",
      "Epoch: 14/20... Step: 1820... Loss: 1.3151... Val Loss: 1.3449\n",
      "Epoch: 14/20... Step: 1830... Loss: 1.3340... Val Loss: 1.3454\n",
      "Epoch: 14/20... Step: 1840... Loss: 1.2831... Val Loss: 1.3421\n",
      "Epoch: 14/20... Step: 1850... Loss: 1.2668... Val Loss: 1.3419\n",
      "Epoch: 14/20... Step: 1860... Loss: 1.3278... Val Loss: 1.3408\n",
      "Epoch: 14/20... Step: 1870... Loss: 1.3306... Val Loss: 1.3345\n",
      "Epoch: 14/20... Step: 1880... Loss: 1.3123... Val Loss: 1.3338\n",
      "Epoch: 14/20... Step: 1890... Loss: 1.3317... Val Loss: 1.3413\n",
      "Epoch: 14/20... Step: 1900... Loss: 1.3081... Val Loss: 1.3365\n",
      "Epoch: 14/20... Step: 1910... Loss: 1.3104... Val Loss: 1.3343\n",
      "Epoch: 14/20... Step: 1920... Loss: 1.3018... Val Loss: 1.3302\n",
      "Epoch: 14/20... Step: 1930... Loss: 1.2726... Val Loss: 1.3330\n",
      "Epoch: 14/20... Step: 1940... Loss: 1.3303... Val Loss: 1.3306\n",
      "Epoch: 15/20... Step: 1950... Loss: 1.2927... Val Loss: 1.3342\n",
      "Epoch: 15/20... Step: 1960... Loss: 1.2896... Val Loss: 1.3319\n",
      "Epoch: 15/20... Step: 1970... Loss: 1.2887... Val Loss: 1.3318\n",
      "Epoch: 15/20... Step: 1980... Loss: 1.2833... Val Loss: 1.3317\n",
      "Epoch: 15/20... Step: 1990... Loss: 1.2842... Val Loss: 1.3278\n",
      "Epoch: 15/20... Step: 2000... Loss: 1.2652... Val Loss: 1.3263\n",
      "Epoch: 15/20... Step: 2010... Loss: 1.2933... Val Loss: 1.3214\n",
      "Epoch: 15/20... Step: 2020... Loss: 1.3072... Val Loss: 1.3246\n",
      "Epoch: 15/20... Step: 2030... Loss: 1.2793... Val Loss: 1.3231\n",
      "Epoch: 15/20... Step: 2040... Loss: 1.2788... Val Loss: 1.3200\n",
      "Epoch: 15/20... Step: 2050... Loss: 1.2743... Val Loss: 1.3217\n",
      "Epoch: 15/20... Step: 2060... Loss: 1.2834... Val Loss: 1.3188\n",
      "Epoch: 15/20... Step: 2070... Loss: 1.2843... Val Loss: 1.3209\n",
      "Epoch: 15/20... Step: 2080... Loss: 1.2777... Val Loss: 1.3186\n",
      "Epoch: 16/20... Step: 2090... Loss: 1.2922... Val Loss: 1.3340\n",
      "Epoch: 16/20... Step: 2100... Loss: 1.2741... Val Loss: 1.3188\n",
      "Epoch: 16/20... Step: 2110... Loss: 1.2753... Val Loss: 1.3141\n",
      "Epoch: 16/20... Step: 2120... Loss: 1.2836... Val Loss: 1.3171\n",
      "Epoch: 16/20... Step: 2130... Loss: 1.2590... Val Loss: 1.3169\n",
      "Epoch: 16/20... Step: 2140... Loss: 1.2611... Val Loss: 1.3120\n",
      "Epoch: 16/20... Step: 2150... Loss: 1.2836... Val Loss: 1.3109\n",
      "Epoch: 16/20... Step: 2160... Loss: 1.2606... Val Loss: 1.3099\n",
      "Epoch: 16/20... Step: 2170... Loss: 1.2644... Val Loss: 1.3092\n",
      "Epoch: 16/20... Step: 2180... Loss: 1.2621... Val Loss: 1.3164\n",
      "Epoch: 16/20... Step: 2190... Loss: 1.2811... Val Loss: 1.3164\n",
      "Epoch: 16/20... Step: 2200... Loss: 1.2604... Val Loss: 1.3093\n",
      "Epoch: 16/20... Step: 2210... Loss: 1.2191... Val Loss: 1.3081\n",
      "Epoch: 16/20... Step: 2220... Loss: 1.2670... Val Loss: 1.3105\n",
      "Epoch: 17/20... Step: 2230... Loss: 1.2454... Val Loss: 1.3082\n",
      "Epoch: 17/20... Step: 2240... Loss: 1.2497... Val Loss: 1.3041\n",
      "Epoch: 17/20... Step: 2250... Loss: 1.2333... Val Loss: 1.3052\n",
      "Epoch: 17/20... Step: 2260... Loss: 1.2426... Val Loss: 1.3043\n",
      "Epoch: 17/20... Step: 2270... Loss: 1.2567... Val Loss: 1.3032\n",
      "Epoch: 17/20... Step: 2280... Loss: 1.2614... Val Loss: 1.3029\n",
      "Epoch: 17/20... Step: 2290... Loss: 1.2579... Val Loss: 1.2983\n",
      "Epoch: 17/20... Step: 2300... Loss: 1.2250... Val Loss: 1.3047\n",
      "Epoch: 17/20... Step: 2310... Loss: 1.2466... Val Loss: 1.3041\n",
      "Epoch: 17/20... Step: 2320... Loss: 1.2499... Val Loss: 1.2998\n",
      "Epoch: 17/20... Step: 2330... Loss: 1.2389... Val Loss: 1.3040\n",
      "Epoch: 17/20... Step: 2340... Loss: 1.2611... Val Loss: 1.2974\n",
      "Epoch: 17/20... Step: 2350... Loss: 1.2577... Val Loss: 1.2958\n",
      "Epoch: 17/20... Step: 2360... Loss: 1.2618... Val Loss: 1.3017\n",
      "Epoch: 18/20... Step: 2370... Loss: 1.2391... Val Loss: 1.2969\n",
      "Epoch: 18/20... Step: 2380... Loss: 1.2430... Val Loss: 1.2977\n",
      "Epoch: 18/20... Step: 2390... Loss: 1.2376... Val Loss: 1.2970\n",
      "Epoch: 18/20... Step: 2400... Loss: 1.2587... Val Loss: 1.2946\n",
      "Epoch: 18/20... Step: 2410... Loss: 1.2605... Val Loss: 1.2916\n",
      "Epoch: 18/20... Step: 2420... Loss: 1.2389... Val Loss: 1.2888\n",
      "Epoch: 18/20... Step: 2430... Loss: 1.2425... Val Loss: 1.2920\n",
      "Epoch: 18/20... Step: 2440... Loss: 1.2328... Val Loss: 1.2921\n",
      "Epoch: 18/20... Step: 2450... Loss: 1.2232... Val Loss: 1.2946\n",
      "Epoch: 18/20... Step: 2460... Loss: 1.2428... Val Loss: 1.2910\n",
      "Epoch: 18/20... Step: 2470... Loss: 1.2419... Val Loss: 1.2913\n",
      "Epoch: 18/20... Step: 2480... Loss: 1.2320... Val Loss: 1.2952\n",
      "Epoch: 18/20... Step: 2490... Loss: 1.2117... Val Loss: 1.2881\n",
      "Epoch: 18/20... Step: 2500... Loss: 1.2323... Val Loss: 1.2903\n",
      "Epoch: 19/20... Step: 2510... Loss: 1.2306... Val Loss: 1.2960\n",
      "Epoch: 19/20... Step: 2520... Loss: 1.2318... Val Loss: 1.2923\n",
      "Epoch: 19/20... Step: 2530... Loss: 1.2468... Val Loss: 1.2927\n",
      "Epoch: 19/20... Step: 2540... Loss: 1.2498... Val Loss: 1.2895\n",
      "Epoch: 19/20... Step: 2550... Loss: 1.2209... Val Loss: 1.2912\n",
      "Epoch: 19/20... Step: 2560... Loss: 1.2299... Val Loss: 1.2860\n",
      "Epoch: 19/20... Step: 2570... Loss: 1.2287... Val Loss: 1.2835\n",
      "Epoch: 19/20... Step: 2580... Loss: 1.2515... Val Loss: 1.2825\n",
      "Epoch: 19/20... Step: 2590... Loss: 1.2110... Val Loss: 1.2819\n",
      "Epoch: 19/20... Step: 2600... Loss: 1.2099... Val Loss: 1.2839\n",
      "Epoch: 19/20... Step: 2610... Loss: 1.2202... Val Loss: 1.2847\n",
      "Epoch: 19/20... Step: 2620... Loss: 1.2054... Val Loss: 1.2853\n",
      "Epoch: 19/20... Step: 2630... Loss: 1.2144... Val Loss: 1.2815\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19/20... Step: 2640... Loss: 1.2296... Val Loss: 1.2872\n",
      "Epoch: 20/20... Step: 2650... Loss: 1.2399... Val Loss: 1.2859\n",
      "Epoch: 20/20... Step: 2660... Loss: 1.2362... Val Loss: 1.2896\n",
      "Epoch: 20/20... Step: 2670... Loss: 1.2350... Val Loss: 1.2823\n",
      "Epoch: 20/20... Step: 2680... Loss: 1.2273... Val Loss: 1.2789\n",
      "Epoch: 20/20... Step: 2690... Loss: 1.2178... Val Loss: 1.2845\n",
      "Epoch: 20/20... Step: 2700... Loss: 1.2294... Val Loss: 1.2752\n",
      "Epoch: 20/20... Step: 2710... Loss: 1.1949... Val Loss: 1.2764\n",
      "Epoch: 20/20... Step: 2720... Loss: 1.2023... Val Loss: 1.2770\n",
      "Epoch: 20/20... Step: 2730... Loss: 1.1892... Val Loss: 1.2765\n",
      "Epoch: 20/20... Step: 2740... Loss: 1.1943... Val Loss: 1.2790\n",
      "Epoch: 20/20... Step: 2750... Loss: 1.1925... Val Loss: 1.2800\n",
      "Epoch: 20/20... Step: 2760... Loss: 1.1912... Val Loss: 1.2832\n",
      "Epoch: 20/20... Step: 2770... Loss: 1.2328... Val Loss: 1.2745\n",
      "Epoch: 20/20... Step: 2780... Loss: 1.2499... Val Loss: 1.2790\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "seq_length = 100\n",
    "n_epochs = 20 # start smaller if you are just testing initial behavior\n",
    "\n",
    "# train the model\n",
    "train(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change the name, for saving multiple files\n",
    "model_name = 'rnn_20_epoch.net'\n",
    "\n",
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "\n",
    "with open(model_name, 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(net, char, h=None, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        \n",
    "        # tensor inputs\n",
    "        x = np.array([[net.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(net.chars))\n",
    "        inputs = torch.from_numpy(x)\n",
    "        \n",
    "        if(train_on_gpu):\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        # detach hidden state from history\n",
    "        h = tuple([each.data for each in h])\n",
    "        # get the output of the model\n",
    "        out, h = net(inputs, h)\n",
    "\n",
    "        # get the character probabilities\n",
    "        p = F.softmax(out, dim=1).data\n",
    "        if(train_on_gpu):\n",
    "            p = p.cpu() # move to cpu\n",
    "        \n",
    "        # get top characters\n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(net.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        # select the likely next character with some element of randomness\n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "        \n",
    "        # return the encoded value of the predicted char and the hidden state\n",
    "        return net.int2char[char], h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None):\n",
    "        \n",
    "    if(train_on_gpu):\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "    \n",
    "    net.eval() # eval mode\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = predict(net, ch, h, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = predict(net, chars[-1], h, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna Alexey Alexandrovitth.\n",
      "\n",
      "\"You want to be all the most any care of the countries.\"\n",
      "\n",
      "\"Yes, I can't think of her, then you met her mother that's his for thriegs to the\n",
      "rand, send me anyone. What are you with tome to those importance in his\n",
      "mother, well it's a string then, who can't such been if these any\n",
      "minuces and my country subject?\" he thought, satting its fresh was the\n",
      "sacondation as he had been to bring him to the\n",
      "consciousness of the comiss, the poor\n",
      "weather, and he felt in tears of her and the peasants, he heard the\n",
      "panding shid in the fronce who was, and showen the place. The\n",
      "soce table was a special pease of an excepress of his fresh and\n",
      "secrets of the sounds and such a possibility of country, had to\n",
      "be serroud at once at all over the chair, had sold her face. But this he\n",
      "had to be destined.\n",
      "\n",
      "And she was started as that has always been always that which had teen her\n",
      "head and have nothing but she had not spent it would help, while\n",
      "a suffering, and he went on. \"I don't understand t\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 1000, prime='Anna', top_k=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here we have loaded in a model that trained over 20 epochs `rnn_20_epoch.net`\n",
    "with open('rnn_20_epoch.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "And Levin said,\n",
      "stopped her, and went up and deconoundly as he could be a candle tatted\n",
      "in the conversation.\n",
      "\"This should be the minutess!\"\n",
      "\n",
      "\"Oh, I was anyway, it's away in the first come,\" said Levin.\n",
      "\n",
      "\"I should be always the motions with him only.\"\n",
      "\n",
      "His brother would have the pail of an early to her true that\n",
      "he was too the same some, and that he would not have here the baly\n",
      "and so that the position had been asking him in a most child, to\n",
      "the seaments of the princess. There had seen her the care of\n",
      "the called an action what he went up, and another carriage,\n",
      "saying what was the fiest of the part to the conversation, and\n",
      "they were through his hands, but the personares, a man that their\n",
      "certain the hand of a man who had seen her. The conversation was\n",
      "consciaution, and who had the close of answering his heart. The\n",
      "princess seemed to her and so much by the sourd of her son he\n",
      "had been so told his back.\n",
      "\n",
      "The same talk transloved his hand. To the semonious strone too, to\n",
      "severathy impossible to tell her, but his condition he wished to trie\n",
      "never with which she had been suffering that he had stop no window to\n",
      "stand the sole stald on the steps to his stands. And he went into the rain\n",
      "the country there had been a mide of the strange and horrist feeling to\n",
      "subject and show his commanss. This secand and whom were\n",
      "silent to the conversation at the time. He sat his head an honest and\n",
      "smile on which they had struck him that all the more so stary that\n",
      "was the more to brought the death of wife and suddenly as she was\n",
      "the principle. An ideas of the secent work they could be in a\n",
      "misutieft, while she could not be so more. He could not have\n",
      "here anything, while he had been to tell it of a portrait of his\n",
      "while.\"\n",
      "\n",
      "\"It's true that they have told hard of my weakness and some\n",
      "ceetance. They say, it's as the most professon.\n",
      "That's the crowd of makally of the coming than the silence\n",
      "is strange, saying you much at once to the second says that I'm\n",
      "never followed anything but the sound that there are that i\n"
     ]
    }
   ],
   "source": [
    "# Sample using a loaded model\n",
    "print(sample(loaded, 2000, top_k=5, prime=\"And Levin said\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
